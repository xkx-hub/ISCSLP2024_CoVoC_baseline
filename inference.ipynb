{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.提取文本特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text import chinese\n",
    "import os \n",
    "import numpy as np\n",
    "finals= ['a','ai','an','ang','ao','e','ei','en','eng','er','i','i0','ia','ian','iang','iao','ie','in','ing','iong','ir','iu','o','ong','ou','u','ua','uai','uan','uang','ui','un','uo','v','van','ve','vn']\n",
    "\n",
    "def generate_token(text):\n",
    "    \n",
    "    phone_set = \"./text/chinese_dict\"\n",
    "    with open(phone_set) as ps:\n",
    "        phone_map = { pho.strip():idx+1 for idx,pho in enumerate(ps.readlines())}\n",
    "\n",
    "    norm_text = chinese.text_normalize(text)\n",
    "    phones, tones, word2ph = chinese.g2p(norm_text)\n",
    "\n",
    "    final_phone_lst = []\n",
    "    for phone,tone in zip(phones,tones):\n",
    "        if '_' == phone:\n",
    "            continue\n",
    "        if phone in finals:\n",
    "            phone = phone+str(tone)\n",
    "        if phone == '!' or phone == '…':\n",
    "            phone = '.'\n",
    "        if phone == '-':\n",
    "            phone = ','\n",
    "        final_phone_lst.append(phone.strip())\n",
    "    if 'iong4' in final_phone_lst:                              # 默认音素集内不存在iong4\n",
    "        assert 0\n",
    "    token_lst = [phone_map[phone] for phone in final_phone_lst]\n",
    "\n",
    "    return np.array(token_lst)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = '然后在家看书的时候，就发现那些什么心灵鸡汤啊、心灵鸡血呀，还有什么毒鸡汤啊，还挺管用的。'    #   提示文本\n",
    "target_text = '你好，我是郭钊。'                                                                #   待合成文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = generate_token(prompt_text)\n",
    "np.save(\"prompt_text_token.npy\",token)\n",
    "token = generate_token(target_text)\n",
    "np.save(\"target_text_token.npy\",token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.以下代码需要在[Amphion](https://github.com/open-mmlab/Amphion)根目录下运行\n",
    "建议将本文件复制到Amphion根目录，具体运行环境见[Amphion](https://github.com/open-mmlab/Amphion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from utils.io import save_audio\n",
    "from tqdm import tqdm\n",
    "from utils.tokenizer import AudioTokenizer,extract_encodec_token\n",
    "from utils.util import load_config\n",
    "from models.tts.valle.valle import VALLE\n",
    "from encodec import EncodecModel\n",
    "from encodec.utils import convert_audio\n",
    "\n",
    "os.environ['WORK_DIR'] = '.'\n",
    "os.environ['PYTHONPATH'] = '.'\n",
    "os.environ['PYTHONIOENCODING'] = 'UTF-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_config = './egs/tts/VALLE/exp_config.json'\n",
    "in_ckpt_path = 'pytorch_model.bin'                          #权重路径\n",
    "\n",
    "\n",
    "cfg = load_config(in_config)\n",
    "model = VALLE(cfg.model)\n",
    "audio_tokenizer = AudioTokenizer()\n",
    "ckpt = torch.load(in_ckpt_path)\n",
    "model.load_state_dict(ckpt)\n",
    "\n",
    "enc_model = EncodecModel.encodec_model_24khz()\n",
    "enc_model.set_target_bandwidth(6.0)\n",
    "enc_model = enc_model.cuda()\n",
    "enc_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_audio_path = 'prompt.wav'                                                    #提示音频路径\n",
    "prompt_text_token_path = 'prompt_text_token.npy'                                    #1中提取的提示文本特征路径\n",
    "target_text_token_path = 'target_text_token.npy'                                    #1中提取的待合成文本特征路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, sr = torchaudio.load(prompt_audio_path)\n",
    "wav = convert_audio(wav, sr, enc_model.sample_rate, enc_model.channels)\n",
    "wav = wav.unsqueeze(0)\n",
    "wav = wav.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoded_frames = enc_model.encode(wav)\n",
    "    codes_ = torch.cat(\n",
    "        [encoded[0] for encoded in encoded_frames], dim=-1\n",
    "    )  # [B, n_q, T]\n",
    "    codes = codes_.cpu().numpy()[0, :, :].T  # [T, 8]\n",
    "\n",
    "prompt_audio_token = codes                          # encodec 特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text_token = np.load(prompt_text_token_path)\n",
    "target_text_token = np.load(target_text_token_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_phone =  np.concatenate((prompt_text_token,target_text_token))\n",
    "semantic_token = torch.from_numpy(all_phone)\n",
    "\n",
    "semantic_len = semantic_token.shape[0]\n",
    "semantic_len = torch.IntTensor([semantic_len])\n",
    "\n",
    "prompt_token = torch.from_numpy(prompt_audio_token)\n",
    "\n",
    "device = 'cuda:0'\n",
    "model = model.to(device)\n",
    "semantic_token = semantic_token.to(device)\n",
    "semantic_len = semantic_len.to(device)\n",
    "prompt_token = prompt_token.to(device)\n",
    "encoded_frames = model.inference(\n",
    "                semantic_token.unsqueeze(0),\n",
    "                semantic_len,\n",
    "                prompt_token.unsqueeze(0),\n",
    "                enroll_x_lens=len(prompt_text_token),\n",
    "                top_k=100,\n",
    "                temperature=1.0,\n",
    "            )\n",
    "samples = audio_tokenizer.decode([(encoded_frames.transpose(2, 1), None)])\n",
    "audio = samples[0].squeeze(0).cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_audio('target.wav', audio, 24000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
